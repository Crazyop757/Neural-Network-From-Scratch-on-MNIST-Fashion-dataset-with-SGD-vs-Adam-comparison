# ðŸ§  Neural Network From Scratch on Fashion-MNIST  
**Comparing SGD vs Adam Optimizers**

---

## ðŸ“Œ Project Overview  
This project is a **from-scratch implementation** of a feedforward neural network (no TensorFlow / PyTorch used).  
The goal is to **classify Fashion-MNIST images** and compare the performance of two optimization algorithms:  
- **Stochastic Gradient Descent (SGD)**  
- **Adam Optimizer**  

Itâ€™s all about understanding how optimizers behave, not just using pre-built libraries.  

---

## ðŸš€ Features  
- Implementation of a neural network **completely from scratch** (only `numpy` used).  
- Training & evaluation on the **Fashion-MNIST dataset**.  
- Comparison between **SGD** and **Adam** optimizers.  
- Visualization of **loss curves & accuracy trends**.  
- Clear, beginner-friendly code structure.  

---

## ðŸ“Š Results  
- **Adam** converges faster and reaches higher accuracy compared to **SGD**.  
- SGD takes longer but helps in understanding the importance of learning rates.  

ðŸ“ˆ Example training curves (Accuracy vs Epochs & Loss vs Epochs) are included in the project.  
